<!DOCTYPE HTML>
<html lang="zh-cn">

<head>
    <!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="流岚雅舍">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <meta name="theme-version" content="1.2.3">
    <meta name="root" content="/">
    <link rel="dns-prefetch" href="https://lankning.github.io">
    <!--SEO-->

<meta name="keywords" content="ML" />


<meta name="description" content="这篇文章使用了Transformer对图像进行了帧合成操作，文章链接如下：
ConvTransformer: A Convolutional Transformer Network for Vi..." />


<meta name="robots" content="all" />
<meta name="google" content="all" />
<meta name="googlebot" content="all" />
<meta name="verify" content="all" />
    <!--Title-->

<title>
    
    论文阅读：ConvTransformer A Convolutional Transformer Network for Video Frame Synthesis |
    
    流岚雅舍
</title>

<link rel="alternate" href="/atom.xml" title="流岚雅舍" type="application/atom+xml">


<link rel="icon" href="/favicon.ico">

    


<link rel="stylesheet" href="/css/bootstrap.min.css?rev=3.3.7.css">
<link rel="stylesheet" href="/css/font-awesome.min.css?rev=4.7.0.css">
<link rel="stylesheet" href="/css/style.css?rev=@@hash.css">

    
<div class="hide">
    <script type="text/javascript">
    var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
    document.write(unescape("%3Cspan class='cnzz_stat_icon_1263868967 hide' %3E%3Cscript%20src%3D%22https%3A%2F%2Fs95.cnzz.com%2Fz_stat.php%3Fweb_id%3D1272564536%22%3E%3C%2Fscript%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s19.cnzz.com/z_stat.php%3Fid%3D1263868967%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
    </script>
</div>




    

<meta name="generator" content="Hexo 5.4.2"></head>
<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->
<body>
    <header class="main-header"  >
    <div class="main-header-box">
        <a class="header-avatar" href="/" title='猪老大'>
            <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block">
        </a>
        <div class="branding">
            <!--<h2 class="text-hide">Snippet主题,从未如此简单有趣</h2>-->
            
            <h2>
                猪老大要进步！
            </h2>
            
        </div>
    </div>
</header>
    <nav class="main-navigation">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav">
                        <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                    <a class="navbar-brand" href="https://lankning.github.io">
                        流岚雅舍</a>
                </div>
                <div class="collapse navbar-collapse" id="main-menu">
                    <ul class="menu">
                        
                        <li role="presentation" class="text-center">
                            <a href="/"><i class="fa "></i>
                                首页</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/学习笔记"><i class="fa "></i>
                                学习笔记</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/小技巧"><i class="fa "></i>
                                小技巧</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/categories/骚操作"><i class="fa "></i>
                                骚操作</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/archives/"><i class="fa "></i>
                                时间轴</a>
                        </li>
                        
                        <li role="presentation" class="text-center">
                            <a href="/about"><i class="fa "></i>
                                关于</a>
                        </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="论文阅读：ConvTransformer A Convolutional Transformer Network for Video Frame Synthesis">
            
            论文阅读：ConvTransformer A Convolutional Transformer Network for Video Frame Synthesis
            
        </h1>
        <div class="post-meta">
    
    <span class="categories-meta fa-wrap">
        <i class="fa fa-folder-open-o"></i>
        <a class="category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a>
    </span>
    
    
    <span class="fa-wrap">
        <i class="fa fa-tags"></i>
        <span class="tags-meta">
            
            <a class="tag-none-link" href="/tags/ML/" rel="tag">ML</a>
            
        </span>
    </span>
    
    
    
    <span class="fa-wrap">
        <i class="fa fa-clock-o"></i>
        <span class="date-meta">
            2021/09/17</span>
    </span>
    
    
</div>
        
        
        <p class="fa fa-exclamation-triangle warning">
            本文于<strong>
                515</strong>
            天之前发表，文中内容可能已经过时。
        </p>
        
    </div>
    
    <div class="post-body post-content">
        <p>这篇文章使用了Transformer对图像进行了帧合成操作，文章链接如下：</p>
<p><a target="_blank" rel="noopener" href="https://www.researchgate.net/publication/346089951_ConvTransformer_A_Convolutional_Transformer_Network_for_Video_Frame_Synthesis/fulltext/5fbb2775458515b797627879/ConvTransformer-A-Convolutional-Transformer-Network-for-Video-Frame-Synthesis.pdf">ConvTransformer: A Convolutional Transformer Network for Video Frame Synthesis</a></p>
<span id="more"></span>

<p>放上划过重点的论文文件：<a href="/assets/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AConvTransformer.assets/ConvTransformer_A_Convolutional_Transformer_Networ.pdf">ConvTransformer</a></p>
<h1 id="Abstract-amp-Introduction"><a href="#Abstract-amp-Introduction" class="headerlink" title="Abstract &amp; Introduction"></a>Abstract &amp; Introduction</h1><p>CNNs在视频帧合成时因为物体变形和移动、环境光改变、相机视角的移动，表现不佳。本文提出了ConvTransformer网络，核心组成要素（core ingredient）是attention layer &amp; multi-head convolutional self-attention，这些要素可以学习视频中帧的顺序依赖。经过实验，这个模型比ConvLSTM模型表现要好。</p>
<h1 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h1><h2 id="VFI"><a href="#VFI" class="headerlink" title="VFI"></a>VFI</h2><p>先说视频帧融合问题是长期存在的，具有挑战性和内在ill-posed的一个问题，因为自然图片和视频是多模态的。Video frame synthesis is a longstanding low-level computer vision problem, which is very challenging and inherently ill-posed since the multi-modal distribution of natural images and videos.</p>
<p>传统使用的是光流法（<strong>optical flow</strong>），但是optical flow对视频中的motion和光变十分敏感。</p>
<p>一些基于深度神经网络的算法替代了光流法，称为<strong>optical based methods</strong>。[18]训练了一个通用CNN来直接合成中间帧，但有时会导致运动模糊；[17]提出了一种3D光流网络，但是面对大尺度运动仍然力不从心；[20,21]将帧插值认为是局部卷积问题，提出了一种CNN对每个像素点都生成空间适应的卷积核，效果不错，但计算量大（suffer from heavy computation），而且面临着同样的对运动敏感的问题。</p>
<p>如今，针对VFI的包括了运动估计和运动补偿的子模块模型被提出，称为<strong>warping based methods</strong>。虽然warping based methods不仅获得了好的插值结果并有无监督运动估计的光明前景，these warping based deep models are mainly developed based on two consecutive frames for frame interpolation, while the <strong>higher-order motion information</strong> of video frame sequence is ignored and not be well exploited.</p>
<p>不同于warping based methods，Vilegas <em>et al.</em> 提出了一种<strong>ConvLSTM based method</strong> MC-Net for extrapolation方法，但是这种方法在两帧有长距离位移时不能建立有效的关联，且由于递归模型导致计算量大，和optical based methods相比表现也不佳。</p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>Transformer是一种为了学习长范围的序列关系设计的一种新架构，开始成功应用于NLP任务如机器翻译、语音识别中。</p>
<p>具体而言，Zihang Dai <em>et al</em>. [8] 为了去除编码长度限制，在模型最后一部分采取了self-attention层建立各部分之间的联系，同时相对位置编码也被包含在他们的工作中。</p>
<p>Wu et al. [33] 减少了Transformer中的channel数，他们认为传统Transformer中有冗余信息，于是introduce了Long-Short Range Attention (LARA)来减少内存占用。</p>
<p>近期， Nicolas <em>et al.</em> 拓展了Transformer用于目标检测并且提出了DETR算法，并在COCO数据集上取得了和Faster RCNN相媲美的结果。DETR算法将二维分解为一维，通过self-attention替代了传统卷积的感受野。但DERT也不能应用于VFI领域，因为VFI和时间、空间高度相关。</p>
<h1 id="Convolutional-Transformer-Architecture"><a href="#Convolutional-Transformer-Architecture" class="headerlink" title="Convolutional Transformer Architecture"></a>Convolutional Transformer Architecture</h1><p><img src="/../../assets/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AConvTransformer.assets/4ejkxf.png" alt="The architecture of the proposed ConvTransformer"></p>
<h2 id="Algorithm-Review"><a href="#Algorithm-Review" class="headerlink" title="Algorithm Review"></a>Algorithm Review</h2><p>输入</p>
<p>$$<br>X&#x3D;{X_0,X_1,…,X_n}\in R^{H×W×C}<br>$$</p>
<p>输出</p>
<p>$$<br>X&#x3D;{X_{i+t_0},X_{i+t_1},…,X_{i+t_k}},t_k\in [0,1]<br>$$</p>
<p>首先，特征嵌入模块嵌入了所有输入视频帧，然后生成对应的特征图feature map。</p>
<p>随后，特征图feature map与位置图positioned map相加得到positioned feature map用于位置标识。</p>
<p>接下来，positioned feature map输入到encoder中建立长范围的序列依赖关系，得到编码后的高级特征图high-level feature map。</p>
<p>再后面，high-level feature map和positioned frame queries同时被传入Decoder中，然后对query frames和 input sequential video frames之间的顺序依赖性进行解码。</p>
<p>最后，得到decoded feature maps，输入到Synthesis Feed-Forward Networks (SFFN)中得到interpolated frames。</p>
<h2 id="Feature-Embedding"><a href="#Feature-Embedding" class="headerlink" title="Feature Embedding"></a>Feature Embedding</h2><p><img src="/../../assets/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AConvTransformer.assets/4Mpe2Q.png" alt="feature embedding"></p>
<p>Feature Embedding模块采用了一个共享的卷积模块，包含了4个LeakyRelu激活的卷积，提取得到了Feature map。</p>
<h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p><img src="/../../assets/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AConvTransformer.assets/4MC8c4.png" alt="positional encodin"></p>
<p>与原始面向向量的Transformer不同的是，此处的positional encodings是一个3D Tensor，与frame feature map有相同的维度，因此两者可以求和。公式如下：<br>$$<br>PosMap_{(p,(i,j,2k))}&#x3D;sin(n&#x2F;10000^{2k&#x2F;d_{model}})\\<br>PosMap_{(p,(i,j,2k+1))}&#x3D;cos(n&#x2F;10000^{2k&#x2F;d_{model}})<br>$$<br>此处，p代表position token，(i,j,2k)代表特征图上的空间位置，n代表输入的序列数量，$d_{model}$代表模型feature map的通道数(?)。</p>
<h2 id="Encoder-amp-Decoder"><a href="#Encoder-amp-Decoder" class="headerlink" title="Encoder &amp; Decoder"></a>Encoder &amp; Decoder</h2><p><img src="/../../assets/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AConvTransformer.assets/4Mk7WV.png" alt="encoder"></p>
<p><strong>Encoder模块</strong>接收positioned feature map（表示为图中的${Z_t}$序列），the encoder is modeled as a stack of N identical layers，每一个stack由两个子层组成：<strong>多头卷积子注意力层</strong>、<strong>简单的2D卷积前馈层</strong>。模型中所有的子层都会采用相同维度的输出$d_{models}&#x3D;32$</p>
<p><img src="/../../assets/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AConvTransformer.assets/43JeVH.png" alt="Decoder"></p>
<p><strong>Decoder模块</strong>也是composed of a stack of N identical layers，每个stack包含了3个子层，除了Encoder中的两个子层，还插入了一个称为<strong>查询自注意（query self-attention）</strong>的附加层，以对输出帧查询执行卷积自注意。</p>
<h2 id="Multi-Head-Convolutional-Self-Attention"><a href="#Multi-Head-Convolutional-Self-Attention" class="headerlink" title="Multi-Head Convolutional Self-Attention"></a>Multi-Head Convolutional Self-Attention</h2><p><img src="/../../assets/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AConvTransformer.assets/43YT10.png" alt="Multi-Head Convolutional Self-Attention"></p>
<p><strong>整个文章的灵魂，最重要的创新点！</strong>可以看到Convolution Self-Attention是使用一个CNN得到Q、K、V三个值（shape为H×W×3）；将所有的K值加上Qi（第i个输入的查询矩阵）再经过CNN得到H(i,j)，即key值j对输入i的attention map；当0&lt;&#x3D;j&lt;n的H(i,j)全部计算出来，就有了其集合$H(i)&#x3D;{H(i,1), H(i,2), · · ·, H(i,n)}$ ，在第3维度上取softmax；将所有输入的H和V对应相加，再将结果相加，输出得到第i个输入的convolutional self-attention；公式表达如下：<br>$$<br>Q_i,K_i,V_i&#x3D;CNN_1(U_i),Q,K,V\in R^{H×W×3}\\<br>H(i,j)&#x3D;CNN_2(Q_t,K_j),H\in R^{H×W×1}\\<br>H(i)&#x3D;Softmax({H(i,1), H(i,2), · · ·, H(i,n)})_d,d&#x3D;3\\<br>V_i&#x3D;sum(V_j+H(i,j))<br>$$<br>使用多个这样的Conv Self-Attention Block并行，就可以得到每一个输入的self-attention矩阵。</p>
<h2 id="Synthesis-Feed-Forward-Network"><a href="#Synthesis-Feed-Forward-Network" class="headerlink" title="Synthesis Feed-Forward Network"></a>Synthesis Feed-Forward Network</h2><p>没啥好说的，是一个unet类型的结构。</p>
<p>In order to synthesize the fifinal photo realistic video frames, we construct a frame synthesis feed-forward network, which consists of 2 cascaded sub-networks built upon a U-Net-like structure.</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.12556">https://arxiv.org/pdf/2012.12556</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/336169274">https://zhuanlan.zhihu.com/p/336169274</a></li>
</ol>

    </div>
    
    <div class="reward" ontouchstart>
    <div class="reward-wrap">赏
        <div class="reward-box">
            
            <span class="reward-type">
                <img class="alipay" src="/img/reward-alipay.jpg"><b>支付宝打赏</b>
            </span>
            
            
            <span class="reward-type">
                <img class="wechat" src="/img/reward-wepay.jpg"><b>微信打赏</b>
            </span>
            
        </div>
    </div>
    <p class="reward-tip">
        赞赏是不耍流氓的鼓励
    </p>
</div>
    
    <div class="post-footer">
        <div>
            
            转载声明：
            商业转载请联系作者获得授权,非商业转载请注明出处 © <a href="mailto:lankning@163.com" target="_blank">猪老大</a>
            
            
        </div>
        <div>
            
        </div>
    </div>
</article>
<div class="article-nav prev-next-wrap clearfix">
    
    <a href="/2021/09/19/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ALearning-Texture-Transformer-Network-for-Image-Super-Resolution/" class="pre-post btn btn-default" title='论文阅读：Learning Texture Transformer Network for Image Super-Resolution'>
        <i class="fa fa-angle-left fa-fw"></i><span class="hidden-lg">上一篇</span>
        <span class="hidden-xs">
            论文阅读：Learning Texture Transformer Network for Image Super-Resolution</span>
    </a>
    
    
    <a href="/2021/09/02/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E4%BD%93%E9%87%8D%E7%A7%B0%E5%88%B6%E4%BD%9C2%EF%BC%9A%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/" class="next-post btn btn-default" title='体重称制作2：系统设计'>
        <span class="hidden-lg">下一篇</span>
        <span class="hidden-xs">
            体重称制作2：系统设计</span><i class="fa fa-angle-right fa-fw"></i>
    </a>
    
</div>

<div id="comments">
    

<div id="vcomments" class="valine"></div>

<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="/assets/valine.min.js"></script>

<script>
new Valine({
    av: AV,
    el: '#vcomments',
    appId: 'iLawMqSXlVorgFJfj2dE1PuS-gzGzoHsz',
    appKey: 'oWij6qQuyzNX3j5eqcwFd37w',
    placeholder: '说点什么吧',
    notify: false,
    verify: true,
    avatar: 'mm',
    meta: 'nick,mail'.split(','),
    pageSize: '10',
    path: window.location.pathname,
    lang: 'zh-cn'.toLowerCase()
})
</script>


</div>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        showProcessingMessages: false,
        messageStyle: "none",
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
            inlineMath:  [ ["$", "$"] ],
            displayMath: [ ["$$","$$"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code','a'],
            ignoreClass:"comment-content"
        },
        "HTML-CSS": {
            availableFonts: ["STIX","TeX"],
            showMathMenu: false
        }
    });
    MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
    </script>
    <script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> 


                </main>
                
                    <aside id="article-toc" role="navigation" class="col-md-4">
    <div class="widget">
        <h3 class="title">
            Table of Contents
        </h3>
        
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Abstract-amp-Introduction"><span class="toc-text">Abstract &amp; Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Related-Works"><span class="toc-text">Related Works</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#VFI"><span class="toc-text">VFI</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer"><span class="toc-text">Transformer</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Convolutional-Transformer-Architecture"><span class="toc-text">Convolutional Transformer Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Algorithm-Review"><span class="toc-text">Algorithm Review</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Feature-Embedding"><span class="toc-text">Feature Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Positional-Encoding"><span class="toc-text">Positional Encoding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Encoder-amp-Decoder"><span class="toc-text">Encoder &amp; Decoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-Head-Convolutional-Self-Attention"><span class="toc-text">Multi-Head Convolutional Self-Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Synthesis-Feed-Forward-Network"><span class="toc-text">Synthesis Feed-Forward Network</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-text">参考资料</span></a></li></ol>
        
    </div>
</aside>
                
            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>
<a id="back-to-top" class="icon-btn hide">
    <i class="fa fa-chevron-up"></i>
</a>
    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="busuanzi">
    
</div>
            </div>
            <div class="col-sm-12">
                <span>Copyright &copy;
                    2017
                    
                </span> |
                <span>
                    Powered by <a href="//hexo.io" class="copyright-links" target="_blank" rel="nofollow">Hexo</a>
                </span> |
                <span>
                    Theme by <a href="//github.com/shenliyang/hexo-theme-snippet.git" class="copyright-links" target="_blank" rel="nofollow">Snippet</a>
                </span>
            </div>
        </div>
    </div>
</div>




<script src="/js/app.js?rev=@@hash.js"></script>

</body>
</html>